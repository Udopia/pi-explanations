\documentclass[a4paper, USenglish, cleveref, autoref, thm-restate]{lipics-v2021}

\bibliographystyle{plainurl}

\usepackage[T1]{fontenc} 
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[ruled,linesnumbered,vlined]{algorithm2e}
\usepackage{xcolor}
\usepackage{tabularx}
\usepackage{tabu}
\usepackage{todonotes}
\usepackage{tabto}
\usepackage{url}
\usepackage{hyperref}
\usepackage{caption} 

\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\DeclareMathOperator\hi{\ensuremath{\mathsf{hi}}}
\DeclareMathOperator\lo{\ensuremath{\mathsf{lo}}}

\theoremstyle{definition}
\newtheorem{encoding}{Encoding}[section]


\title{Explaining SAT Benchmark Classifiers with \\Prime Implicants}
\author{Markus Iser}{Karlsruhe Institute of Technology (KIT), Institute of Theoretical Informatics, Algorithm Engineering, Germany \and \url{https://algo2.iti.kit.edu/english/3986.php}}{markus.iser@kit.edu}{https://orcid.org/0000-0003-2904-232X}{}
\Copyright{Markus Iser}
\authorrunning{M. Iser}
\ccsdesc{Theory of computation~Logic and verification}
\ccsdesc{Computing methodologies~Supervised learning by classification}
\keywords{Benchmarks, Explainable Algorithm Selection, Prime Implicants}

\begin{document}
\maketitle
\begin{abstract}
Taxonomies for benchmark instances can be derived in several ways. For example, this can be based on their theoretical properties, or it can be based on their origin, e.g., a concrete application. 
Some algorithm selectors even generate instance classes based on a set of features in an unsupervised manner. 
We see a gap in the explainability of such approaches. 
In this paper, we present a SAT encoding for decision tree and random forest classifiers that we can use to generate prime implicants. 
We then apply this encoding to generate minimal explanations for two types of classifiers. 
The first type of classifier was trained to map SAT instances to their instance family.
The second type of classifier was trained as an instance-specific SAT solver selector on a small portfolio.
We show that the explainability of machine learning methods is useful for interpreting experiments as it provides valuable feedback to the algorithm developer.
\end{abstract}


%%%%%%%
\section{Introduction}

Models created by inductive learning algorithms are useful in practice.
The explainability of such automatically generated models is the subject of current research projects~\cite{Audemard:2021:Intelligibility, Darwiche:2020:Reasons}.
In particular, for many application scenarios where accountability concerns arise, it is imperative that what is learned can be explained~\cite{Percy:2021:Accountability}. 
%Examples include ``racist hand dryer'', tanks vs. mornings, security relevant areas like auto pilots of airplanes and cars...

Automated methods for generating explanations for machine-learned models include formalizing and encoding them in formal logic. 
The symbolic representation of what is learned enables the application of deductive methods for reasoning about what is learned.
Deductive reasoning about inductively generated models is usually referred to by the term eXplainable AI (XAI)~\cite{Arrieta:2019:XAI}. 

The term \emph{explanation} is ambiguous. 
There is a plethora of possible formalizations leading to different definitions of explainability. 
Audemard et al. analyze the complexity of different types of XAI queries~\cite{Audemard:2020:XAI}. 
One of the explanatory queries they analyze is prime implicants. 

Choi et al. present a coding of decision trees and random forests for computing prime implicants~\cite{Choi:2020:RF}. 
They report limitations in encoding cardinality constraints for multivalued variables. 

Inductive learning methods have been used in various ways to solve SAT problems more efficiently. 
Cherif et al. successfully used reinforcement learning to control the switching of branching heuristics in their solver Kissat~MAB~\cite{Cherif:2021:KissatMAB}. 
Clause forgetting heuristics based on classification and regression using solver runtime data were presented by Soos et al. with their system Crystal Ball~\cite{Soos:2019:CrystalBall}.

Prediction models have also been used to create algorithm selectors for portfolios of SAT solvers (cf. SATzilla~\cite{Xu:2008:SATzilla}). 
In their approach known as instance-specific algorithm configuration (ISAC), Kadioglu et al. use unsuperived learning to create clusters in the feature space of SAT instances~\cite{Kadioglu:2010:ISAC}. 
For each cluster, a configurator optimizes a SAT solver for the instances in that cluster. 

Prediction models for algorithm selection induce, in a sense, a taxonomy for SAT instances. 
This differs from the classes used in common practice, where instances are usually assigned to an instance family~\cite{Froleyks:2021:SC2020}. 
The instance family may for example describe a category of SAT applications (such as \emph{hardware verification}), or refer to a specific class of instances that is of theoretical interest (such as \emph{pigeon hole}). 

Elffers et al. analyze SAT solver configurations for instance classes of theoretical interest~\cite{Elffers:2018:Insights}. 
Audemard and Simon devise a taxonomy which is purely based on solver runtime parameters and deduce class specific solver configurations~\cite{Audemard:2016:Extreme}. 

Given the circumstances sketched so far, we conclude that explanations of prediction models for SAT benchmarks can provide valuable feedback to the algorithm engineer about what was learned. 
Such explanations allow the algorithm engineer to reason about the induced instance taxonomy, and to improve their algorithm or configuration choices. 

In this paper, we present a simple encoding of decision trees classifiers and random forests as a monotone combinatorial circuit. 
We show how prime implicants can be computed for the resulting formula using an off-the-shelf incremental SAT solver. 
Moreover, we present a tool which can encode decision tree classifiers generated with the Python package \verb!scikit-learn!~\cite{Pedregosa:2011:Scikit} into propositional formulas. 
Our tool uses the SAT solver \verb!CaDiCaL!~\cite{Biere:2020:Cadical} via its \verb!IPASIR!~\cite{Balyo:2016:Ipasir} interface to generate prime implicants. 
The prime implicants are then decoded with respect to the represented case-distinctions in the features space. 
We evaluate the tool on a set of 26794 SAT instances, with a large set of features including their instance families. 

The document is structured as follows. 
In Section~\ref{sec:prelim}, we present a formalization of decision tree classifiers and random forest classifiers as they are implemented in the Python package \verb!scikit-learn!. 
The section concludes with the required fundamentals on propositional logic, incremental SAT solvers and prime implicants. 
In Section~\ref{sec:approach}, we present a propositional encoding of the previously formalized classifiers and an algorithm to compute prime implicants for them. 
We present our implementation in Section~\ref{sec:evaluation}, where we also evaluate our approach on a classifier which was trained to associate SAT instances families for a set of 26794 SAT instances based on a set of instance features which we also describe in that section. 
We also evaluate our approach on a classifier which was trained to select SAT solvers from small portfolios of solvers drawn from SAT competition~2020. 
We conclude with Section~\ref{sec:conclusion}. 


%%%%%%%
\section{Preliminaries}
\label{sec:prelim}

In the following sections, we formally describe the data structures created by \emph{decision tree} (Section~\ref{sec:prelim:dt}) and \emph{random forest} (Section~\ref{sec:prelim:rf}) classifiers which realize the learned prediction function as they are implemented in the Python package \verb!scikit-learn!~\cite{Pedregosa:2011:Scikit}. 
In Section~\ref{sec:prelim:pi}, we introduce some basic notions of propositional logic in particular regarding prime implicants. 
The formalizations will later serve as a fundament to the propositional encoding of the prediction models under consideration (Section~\ref{sec:approach}). 

\subsection{Classification}
\label{sec:prelim:classification}

The \emph{classification problem} under consideration is specified as follows. 
Given a set of training samples $T \subset \mathbb{R}^n$, a set of classes $K$, and a functional ground-truth relation $G \subset T \times K$, devise a prediction function $c : \mathbb{R}^n \rightarrow K$ which maximizes the cardinality of correctly classified training samples $\bigl|\{ t \in T \mid (t, c(t)) \in G \}\bigr|$. 
The challenge is to devise a prediction function which generalizes to yet unseen samples of the feature space. 
In theory, decision trees can grow until they classify each training sample correctly, i.e., the classification result equals ground truth. 
In practice however, pruning techniques such as maximum depth or minimum leaf size are used in order to improve the generalization of a classifier to yet unseen samples. 
An introduction to methods and challenges of classification tasks and supervised learning in general can be found in Hastie et al.~\cite{Hastie:2009}. 
The following formalization depends on the concrete implementations \verb!tree.DecisionTreeClassifier! and \verb!ensemble.RandomForestClassifier! in the Python package \verb!scikit-learn!~\cite{Pedregosa:2011:Scikit}.

\subsubsection{Decision Tree Classifiers}
\label{sec:prelim:dt}

\newcommand{\innernodes}{\ensuremath{V^{\!I}}}
\newcommand{\leafnodes}{\ensuremath{V^{\!L}}}
\newcommand{\edgesH}{\ensuremath{E^+}}
\newcommand{\edgesL}{\ensuremath{E^-}}

Let a classification instance $(T, K, G)$ and its solution in form of a \emph{decision tree} $\mathcal{D}$ be given. 
A decision tree $\mathcal{D} = (V, E, f, t)$ is specified by a \emph{binary tree} with nodes $V$ and edges $E \subset V \times V$. 
The nodes are partitioned into \emph{inner nodes} $\innernodes := \{ v \in V \mid \exists x, (v,x) \in E\}$, and \emph{leaf nodes} $\leafnodes = V \setminus \innernodes$. 
The \emph{root node} $r$ is the special inner node with $\nexists x, (x,r) \in E$. 
The set of edges $E$ is partitioned into \emph{positive edges} $\edgesH$ and \emph{negative edges} $\edgesL$ such that each inner node $v \in \innernodes$ has exactly one \emph{positive successor} $\hi(v) \in V$ with $\bigl(v, \hi(v)\bigr) \in \edgesH$ and exactly one \emph{negative successor} $\lo(v)$ with $\bigl(v, \lo(v)\bigr) \in \edgesL$. 
Associated with each inner node $v \in \innernodes$ is a feature index $f : \innernodes \rightarrow \{1, 2, \dots, n\}$ and a threshold $t : \innernodes \rightarrow \mathbb{R}$. 

Given a set of samples $S \subseteq \mathbb{R}^n$, the decision tree $\mathcal{D}$ induces a partitioning $P_S : V \rightarrow 2^S$ of samples over nodes in $V$. 
Starting with $P_S(r) = S$ for root node $r$, the partitioning is recursively defined as in Equation~\ref{eq:partitioning}. 
\begin{align}
\label{eq:partitioning}
P_S(v) = \begin{cases}
S & \text{iff v is the root node}\\
\bigl\{(s_0, \dots, s_n)^{\intercal} \in P_S(x) \mid s_{\!f\!(\!x\!)} \leq t(x)\bigr\} & \text{iff }v=\hi(x)\\
\bigl\{(s_0, \dots, s_n)^{\intercal} \in P_S(x) \mid s_{\!f\!(\!x\!)} > t(x)\bigr\} & \text{iff }v=\lo(x)
\end{cases}
\end{align}

The partitioning $P_T$ of the set of training samples $T$ together with ground truth $G$ induces for each node $v$ a probabilty distribution $p_v$ over classes $k \in K$ as in Equation~\ref{eq:probability}. 
\begin{align}
\label{eq:probability}
p_v(k) = \frac{\bigl| \{ s \in P_T(v) \mid (s, k) \in G \} \bigr|}{\bigl|P_T(v)\bigr|}
\end{align}

The decision tree $\mathcal{D}$ specifies a classification function $c(s) : \mathbb{R}^n \rightarrow K$. 
Given a sample $s \in \mathbb{R}^n$, there exists exactly one leaf node $v \in \leafnodes$ such that $s \in P_{\!\{\!s\!\}}(v)$. 
The classification result for sample $s$ is the one with the highest probability in that node, i.e., $c(s) = \arg\max\limits_{k \in K} p_v(k)$. 


\subsubsection{Random Forest Classifiers}
\label{sec:prelim:rf}

In the context of machine learning, \emph{bagging} is the idea to combine several weak learners to one stronger learner by using some kind of voting to determine one collective prediction~\cite{Hastie:2009}. 
A \emph{random forest} $\mathcal{R}^d = \{ (T_i, \mathcal{D}_i) \mid 1 \leq i \leq d \}$ combines a set of $d$ decision trees. 
Each decision tree $\mathcal{D}_i$ is independently trained on randomly selected subsets of the training samples $T_i \subset T$. 

Let a classification problem $(T, K, G)$ and a random forest $\mathcal{R}^d = \{ (T_i, \mathcal{D}_i) \mid 1 \leq i \leq d \}$ be given. 
For each decision tree $\mathcal{D}_i = (V_i, E_i, f_i, t_i)$, the training samples $T_i$ induce a probability distribution $p_v(k)$ for nodes $v \in V_i$ (cf. Section~\ref{sec:prelim:dt}). Given a sample $s \in \mathbb{R}^n$, for each decision tree $\mathcal{D}_i$ there exists exaclty one leaf node $v_i \in \leafnodes_i$ such that $s \in P_{\!\{\!s\!\}}(v_i)$. 
The collective class probabilities $p'(k)$ for a sample $s$ are determined by averaging over the class probablities in each tree as in Equation~\ref{eq:probarf}. 
The classification result for sample $s$ is then given by $c'(s) = \arg\max\limits_{k \in K} p'(k)$. 

\begin{align}
\label{eq:probarf}
p'(k) = \frac{1}{d}\sum\limits_{i = 1}^d p_{v_i}(k)
\end{align}


\subsection{Prime Implicants}
\label{sec:prelim:pi}

In this document, propositional formulas are given in conjunctive normal form (CNF). 
A propositional \emph{formula} $F$ is defined over a finite set of Boolean variables $V$. 
Each formula is a conjunction of clauses, a clause is a disjunction of literals and a literal is either a variable or its negation.  
A set of (non-contradictory) literals over $V$ is a \emph{model} $M$ of a formula $F$, iff its intersection with any clause in $F$ is non-empty. 
A model is \emph{complete} iff $|M| = |V|$ and \emph{partial} otherwise. 
Each model $M$ of a formula $F$ is also an implicant of $F$, denoted by $M \models F$. 
An implicant $M$ is a prime implicant of $F$ iff $\nexists M' \subset M, M' \models F$. 

Prime implicants for CNF formulas can efficiently be calculated through eager minimization of a model with an incremental SAT solver~\cite{Iser:2013:Minimizing,Iser:2020:Disse}. 
If the CNF formula encodes a \emph{monotonic} combinational circuit, it is suffient to minimize the model for the input variables of that circuit, as a complete assignment can later be deduced from the minimized input assignment~\cite{Iser:2020:Disse}. 


%%%%%%%
\section{Approach}
\label{sec:approach}

In the following we describe CNF encodings for decision tree classifiers (Section~\ref{sec:approach:dt}) and random forests (Section~\ref{sec:approach:rf}). 
The encodings resemble monotonic circuits which constrain feature values at their inputs. 
In Section~\ref{sec:approach:pi}, we outline an algorithm to efficiently compute prime implicants for these formulas. 
In Section~\ref{sec:approach:dec}, we show how to decode these prime implicants in order to map them to a set of case distinctions in the feature space. 

\subsection{Encoding Decision Tree Classifiers}
\label{sec:approach:dt}

Given a classification instance $(T, K, G)$ and a corresponding decision tree $\mathcal{D}=(V, E, f, t)$, we create its propositional encoding as follows. 

\subsubsection{Variables} 

\newcommand{\classv}[1]{\alpha(#1)}
\newcommand{\nodehv}[1]{\beta^+(#1)}
\newcommand{\nodelv}[1]{\beta^-(#1)}
\newcommand{\interv}[2]{\gamma(#1,#2)}

We introduce three sets of Boolean variables ($\alpha, \beta$, and $\gamma$), which together form the set of variables $V$ over which our encoding $F$ is defined. 
For each class $k \in K$, we introduce a \emph{class variable} $\classv{k}$ for indicating the classification result. 
For each inner node $v \in \innernodes$, we introduce two \emph{node variables} $\nodehv{v}$ and $\nodelv{v}$, with $\nodehv{v}$ denoting its successor $\hi(v)$, and $\nodelv{v}$ denoting its successor $\lo(v)$. 
For each feature $f$, we construct the auxiliary set of thresholds $\sigma_f$ as specified by Equation~\ref{eq:gamma-aux}. 
From $\sigma_f$ we construct the auxilary set of threshold intervals $\tau_f$ as shown in Equation~\ref{eq:gamma-int}. 
%
\begin{align}
\label{eq:gamma-aux}
\sigma_f &:= \{ t(v) \mid f(v) = f, v \in V \} \cup \{-\infty, \infty\}\\
\label{eq:gamma-int}
\tau_f &:= \{ (t_0, t_1] \mid t_0, t_1 \in \sigma_f \land t_0 < t_1 \land \nexists t' \in \sigma_f, t_0 < t' < t_1 \}
\end{align}
%
For each feature $f$ and each threshold interval $z \in \tau_f$, we introduce the Boolean variable $\interv{f}{z}$. 
Variables of type $\interv{f}{z}$ indicate whether the respective interval is excluded by the decision tree. 
This type of value encoding has previously been proposed~\cite{Choi:2020:RF}. 

\subsubsection{Clauses}

%For each class $k \in K$, we determine the leaf nodes in which the classifier outputs $k$, and create the two auxiliary sets of their parent nodes as depicted in Equations~\ref{eq:class-aux1} and~\ref{eq:class-aux2}. 
%
%\begin{align}
%\label{eq:class-aux1}
%V^+_k &:= \bigl\{ v \mid \hi(v) \in \leafnodes \land k=\arg\max\limits_{k \in K} p_{\hi(v)}(k) \bigr\}\\
%\label{eq:class-aux2}
%V^-_k &:= \bigl\{ v \mid \lo(v) \in \leafnodes \land k=\arg\max\limits_{k \in K} p_{\lo(v)}(k) \bigr\}
%\end{align}
%
%Then for each class $k$, we encode a clause which we denote as the \emph{class constraint} for class $k$ as shown in Encoding~\ref{enc:class}. 
%
%\begin{encoding}[Class Constraints]
%\label{enc:class}
%\begin{align*}
%\classv{k} \rightarrow \bigvee\limits_{v \in V^+_k} \nodehv{v} \vee \bigvee\limits_{v \in V^-_k} \nodelv{v}
%\end{align*}
%\end{encoding}

For each class $k \in K$, we determine the set of leaf nodes $\leafnodes_k \subseteq \leafnodes$ in which the classifier outputs $k$. 
Then we encode the \emph{class constraint} for each class $k$ as depicted in Encoding~\ref{enc:class}. 
%
\begin{encoding}[Class Constraints]
\label{enc:class}
\begin{align*}
\forall k \in K, \classv{k} \rightarrow \bigvee\limits_{v \in \leafnodes_k} \nodehv{v}
\end{align*}
\end{encoding}
%
For each inner node $v \in \innernodes$, we encode the \emph{node constraints} as shown in Encoding~\ref{enc:node}. 
%
\begin{encoding}[Node Constraints]
\label{enc:node}
\begin{align*}
\forall v \in \innernodes, \nodehv{\hi(v)} &\rightarrow \nodehv{v}\\ 
\forall v \in \innernodes, \nodelv{\hi(v)} &\rightarrow \nodehv{v}\\ 
\forall v \in \innernodes, \nodehv{\lo(v)} &\rightarrow \nodelv{v}\\ 
\forall v \in \innernodes, \nodelv{\lo(v)} &\rightarrow \nodelv{v}
\end{align*}
\end{encoding}
%
For each inner node $v \in \innernodes$, we encode its \emph{interval constraints} as follows. 
We split the set of threshold variables $\tau_{f(v)}$ and into two auxiliary sets $\tau^+(v)$ and $\tau^-(v)$ which are defined as follows. 
%
\begin{align*}
\tau^+(v) &:= \{ (t_0, t_1] \in \tau_{f(v)} \mid t_1 \leq t(v) \}\\
\tau^-(v) &:= \{ (t_0, t_1] \in \tau_{f(v)} \mid t_0 > t(v) \}
\end{align*}
%
Then we encode the \emph{interval constraints} as in Encoding~\ref{enc:inter}. 
%
\begin{encoding}[Interval Constraints]
\label{enc:inter}
\begin{align*}
\forall v \in \innernodes, \bigwedge\limits_{\tau \in \tau^-(v)} \nodehv{v} \rightarrow \gamma(v, \tau)\\
\forall v \in \innernodes, \bigwedge\limits_{\tau \in \tau^+(v)} \nodelv{v} \rightarrow \gamma(v, \tau)
\end{align*}
\end{encoding}


\subsection{Encoding Random Forest Classifiers}
\label{sec:approach:rf}

Let a random forest $\mathcal{R}^d = \{ (T_1, \mathcal{D}_1), \dots, (T_d, \mathcal{D}_d) \}$ be given. 
Each decision tree $\mathcal{D}_i$ was trained with the classification instance $(T_i, K, G)$ and is given by $\mathcal{D}_i=(V_i, E_i, f_i, t_i)$. 

The encoding for each decision tree is similar to the encoding described in Section~\ref{sec:approach:dt}. 
In particular, the node constraints given by Encoding~\ref{enc:node} are just the same. 
But as in a random forest, classes are not simply determined by a singular leaf nodes, we need a different encoding for class constraints (cf.~Section~\ref{sec:class-rf}). 
Moreover, we need to adapt the interval encoding for reasons which we will explain in the following section. 


\subsubsection{Encoding Interval Constraints for Random Forests}
\label{sec:interval-rf}

As thresholds are chosen independently the size of the threshold set $\xi=|\{ t(v_i) \mid f(v_i) = f, v_i \in V_i, 1 \leq i \leq d \}|$ can be very large for a given feature $f$. 
In preliminary experiments with $100$ decision trees $\xi$ could go into the thousands. 
The number of interval clauses generated when using Encoding~\ref{enc:inter} is in $\mathcal{O}(\xi^2)$. 
We therfore use a new encoding for interval contraints which has a higher \emph{constant} size overhead in terms of variables as well as clauses but scales with $\mathcal{O}(\xi)$. 

\newcommand{\interhv}[2]{\epsilon^+(#1,#2)}%
\newcommand{\interlv}[2]{\epsilon^-(#1,#2)}%
For each feature $f$ with intervals $\tau_f$ (defined in Equation~\ref{eq:gamma-int}), we introduce two new sets of variables $\interhv{f}{z}$ and $\interlv{f}{z}$ with $z \in \tau_f$. 
The interval order of the $z_i \in \tau_f$ also implies an ordering of the variables $\interhv{f}{z_i}$ and $\interlv{f}{z_i}$. 
We first encode for each feature $f$ the \emph{connect constraints}, as depicted in Encoding~\ref{enc:connect}. 
%
\begin{encoding}[Interval Connect Constraints]
\label{enc:connect}
\begin{align*}
\forall f \in [1, \dots, n], \forall z_i \in \tau_f, \interlv{f}{z_i} &\rightarrow \interv{f}{z_i}\\
\forall f \in [1, \dots, n], \forall z_i \in \tau_f, \interhv{f}{z_i} &\rightarrow \interv{f}{z_i}
\end{align*}
\end{encoding}
%
We further introduce the following \emph{order constraints}. 
%
\begin{encoding}[Order Constraints]
\begin{align*}
\forall f \in [1, \dots, n], z_i \in \tau_f, \interhv{f}{z_1} \rightarrow \dots \rightarrow \interhv{f}{z_{|\tau_f|}}\\
\forall f \in [1, \dots, n], z_i \in \tau_f, \interhv{f}{z_1} \leftarrow \dots \leftarrow \interhv{f}{z_{|\tau_f|}}
\end{align*}
\end{encoding}
%
Note that, the encoding still resembles a monotonic combinational circuit, which makes it easier to compute prime implicants of the classifier. 


\subsubsection{Encoding Class Constraints for Random Forests}
\label{sec:class-rf}

Recapitulate how classification works in the random forests described in Section~\ref{sec:prelim:rf}. 
Given a sample $s$, for each decision tree $\mathcal{D}_i$ with leaf nodes $\leafnodes_i$, there is exactly one leaf node $v_i \in \leafnodes_i$ such that $s \in P_{\!\{\!s\!\}}(v_i)$. 
Given the tuple of leaf nodes $(v_1, \dots, v_d)$, i.e., one for each tree, the class of $s$ is determined by the maximum average class probability over those leaf nodes as depicted in Equation~\ref{eq:probarf}. 

One way to encode this, would be to encode an arithmetic circuit.%
\footnote{In this circuit, for each decision tree in the forest, we would encode a list of bit-vectors representing the probabilities of each class, which are encoded to be equivalent to the known class probabilities -- dependent on the activated leaf node. 
For each class, we would further encode an adder circuit to calculate the sum of each classes probabilities over all trees in the forest. 
On top of that, we would then encode a circuit which ensures that the sum of probabilities of the class to explain is the maximum of all classes.}
For the implementation, which we present in this paper, we took a simpler approach. 
Consider again the tuples of leaf nodes $L := \leafnodes_1 \times \dots \times \leafnodes_d$. 
To each tuple in $t \in L$, we can assign a class in $K$ by determining the maximum average class probability for the leaf nodes. 
But the size of $L$ grows exponentially with the number of decision trees in the forest. 
Fortunately, only a fraction of tuples in $L$ is actually a valid combination of leaf nodes, i.e., they also satisfy the node and interval constraints. 

\paragraph{Determining Valid Leaf Tuples}

\newcommand{\validtuples}{\ensuremath{L'}}%
\newcommand{\tuplev}[1]{\ensuremath{\delta(#1)}}%
In order to determine the set of valid tuples $\validtuples \subseteq L$, we encode a small auxiliary SAT problem. 
We use the encoding described in the previous section and \emph{validity constraints} as depicted in Figure~\ref{enc:validity}. 
This ensures that the tuples allow for at least one interval per feature. 
%
\begin{encoding}[Validity Constraints]
\label{enc:validity}
\begin{align*}
\forall f \in [1, \dots, n], \bigvee\limits_{z_i \in \tau_f} \lnot \interv{f}{z_i}
\end{align*}
\end{encoding}
%
We then enumerate all solutions to variables in $\{ \nodehv{v} | v \in V_L \}$


\paragraph{Connection Classes to Tuples}

For each tuple in $t_i \in \validtuples$, we introduce a new variable $\tuplev{t_i}$ and encode the \emph{tuple constraint} as depicted in Encoding~\ref{enc:tuple}. 
%
\begin{encoding}[Tuple Constraints]
\label{enc:tuple}
\begin{align*}
\forall t_i \in L', \bigwedge\limits_{v \in t_i} \tuplev{t_i} \rightarrow \nodehv{v}
\end{align*}
\end{encoding}
%
For each tuple $t_i$, we determine its class $k_i \in K$ and encode the class constraints as depicted in Encoding~\ref{enc:class-rf}. 
%
\begin{encoding}[Class Constraints]
\label{enc:class-rf}
\begin{align*}
\forall k_i \in K, \classv{k_i} \rightarrow \bigvee\limits_{t_i \in \validtuples} \tuplev{t_i}
\end{align*}
\end{encoding}


\subsection{Computing Prime Implicants}
\label{sec:approach:pi}

Given a classifier, we encode it as described above. 
Then we add an explanation clause with class variables for the classes which we want shortest explanations for. 
In most cases, the explanation clause might be a unit clause, since we often want shortest explanations for a single class. 

The encoding of a decision tree resembles a monotonic combinatorial circuit. 
It is rooted in the explanation clause of the classes to explain. 
The inputs to this circuit are represented by the interval variables. 
Due to monotonicity, we can compute prime implicants by minimizing assignments to the interval variables. 

Algorithm~\ref{algo:prime} outlines the procedure. 
The algorithm receives as input the formula encoding the classifier including the explanation clause, and the set of input variables which is comprised of all interval variables. 


\begin{algorithm}
\caption{Incremental Computation of Prime Implicants}
\label{algo:prime}
\DontPrintSemicolon

\KwIn{CNF Formula: $F$}
\KwIn{Input Variables: $I$}
\KwOut{Prime Implicants: $P$}

\SetKwData{A}{assumptions}
\SetKwData{R}{sat}
\SetKwData{S}{model}
\SetKwData{I}{minim}
\SetKwFunction{solve}{solve}

\BlankLine

$(\R,\S) \leftarrow \solve(F, \emptyset)$ \;
\While {\R} {
	\While {\R} {
		$\A \leftarrow \emptyset$ \;
		$\I \leftarrow \emptyset$ \;
		\For {$v \in I$} {
			\If {$v \in \S$} {
				$\I \leftarrow \I \cup \{ -v \}$ \;
			}
			\Else {
				$\A \leftarrow \A \cup \{ -v \}$ \;
			}
		}
		$F \leftarrow F \cup \I$ \;
		$(\R,\S) \leftarrow \solve(F, \A)$ \;
		\If {\textsf{not} \R} {
			$P \leftarrow P \cup \{ -v \mid v \in \I \}$ \;
		}
	}
	$(\R,\S) \leftarrow \solve(F, \emptyset)$ \;
}
\Return $P$ \;
\end{algorithm}


%\subsection{Decoding Prime Implicants}
%\label{sec:approach:dec}


%%%%%%%
\section{Evaluation}
\label{sec:evaluation}


\subsection{Implementation and Dataset}
\label{sec:evaluation-impl}

An implementation of our approach can be found on GitHub.\footnote{\url{https://github.com/Udopia/pi-explanations}}
It consists of a \verb!Python! module written in \verb!C++! which computes prime implicants (cf. Section~\ref{sec:approach:pi}) using the incremental SAT solver \verb!CaDiCaL! by Armin Biere~\cite{Biere:2020:Cadical}. 
The encoding in implemented in \verb!Python! for the decision tree classifier implementation in the \verb!scikit-learn! package~\cite{Pedregosa:2011:Scikit}. 


\subsubsection{Dataset}
\label{sec:eval:data}

Our implementation accesses data via our \verb!gbd-tools! package\footnote{\url{https://pypi.org/project/gbd-tools/}} which we orginially presented in~\cite{Iser:2018:GDB}. 
The three datasets which we used in our evaluations can be obtained at \url{https://gbd.iti.kit.edu/} and are described in the following. 

\paragraph{Meta Features}

We collected a set of meta features in a huge database of more than 26k benchmark instances (mainly from SAT competitions since 2002). 
These meta fetures include the sat/unsat result (if known) and the instance family. 
The instance families have been manually deduced from the documents in SAT competition proceedings or SAT competition presentation slides. 


\paragraph{Base Features}

\subparagraph{Amounts}

This set of features includes the number of \textsf{clauses}, \textsf{variables}, the numbers of clauses of \textsf{sizes 1 to 9}, the number of \textsf{horn clauses}, \textsf{inverse horn clauses}, \textsf{positive clauses}, and \textsf{negative clauses}. 

\subparagraph{Distribution over Horn Clauses}

For this set of features, we count for each variable its number of occurences in horn clauses. 
The counts are represented by five features, their \textsf{mean}, \textsf{variance}, \textsf{minimum}, \textsf{maximum} and \textsf{entropy}. 
We add another five features for the count of variable occurences in inverse horn clauses. 

\subparagraph{Balance of Literal Polarities}

Here we report on two distributions in terms of their \textsf{mean}, \textsf{variance}, \textsf{minimum}, \textsf{maximum} and \textsf{entropy}. 
The first distribution captures for each variable the fraction of its number of positive occurences in clauses divided by the number of negative occurences in clauses. 
The second distribution captures for each clause the number of positive literals divded by the number of negative literals. 

\subparagraph{Distribution of Node Degrees}

Degree distributions by \textsf{mean}, \textsf{variance}, \textsf{minimum}, \textsf{maximum} and \textsf{entropy} for nodes in the variable interaction graph, nodes in the clause graph, variable nodes in the variable-clause graph, clause nodes in the variable-clause graph. 


\paragraph{Gate Features}

\subparagraph{Amounts}
total number of gates, number of root variables, number of input variables, number of generically recognized gates, number of monotonically nested gates, number of non-monotonically nested and-gates, number of non-monotonically nested or-gates, number of non-monotonically nested trivial equivalence gates, number of non-monotonically nested equiv- or xor-gates, number of non-monotonically nested full gate (=maxterm encoding) with more than two inputs

\subparagraph{Distribution over Levels}
For each gate type we also determine their levels in the decoded hierarchical gate structure, for each type we report on the distribution of levels by their \textsf{mean}, \textsf{variance}, \textsf{minimum}, \textsf{maximum} and \textsf{entropy}.


\subsection{Explaining Classification Results}

\begin{itemize}
\item less case distinctions (leaf node level vs. number of case distinctions by prime implicant)
\item only very few features responsible (no need to calculate them all)
\item few prime implicants cover most of the samples
\item some prime implicants for single sample
\end{itemize}


\subsubsection{Instance Family Classifier}

Train classifier to predict instance family for an instance given 26794 instances of SAT Competitions. 

\begin{itemize}
\item Number of instances: 26794
\item Number of instance families: 134
\item Classifier Accuracy (5-fold): 0.97
\end{itemize}


\begin{figure}
todo
\caption{Number of Case Distinctions needed for Instance Family Classification: Decision Tree vs. Decoded Prime Implicants}
\end{figure}



\subsubsection{Small Portfolios Classifier}

Froleyks et al. report on best small portfolios drawn from solvers of SAT Competition~2020~\cite{Froleyks:2021:SC2020}. 
Train classifier to predict fastest solver for an instance given 400 instances of SAT Competition~2020. 

\begin{itemize}
\item Number of instances: 400
%\item Number of solvers in portfolio: 2
\item Best 2-Portfolio: kissat unsat, relaxed newtech
\item Classifier Accuracy (5-fold): 0.67
\end{itemize}

\begin{figure}
todo
\caption{Number of Case Distinctions for Small Portfolio Classification: Decision Tree vs. Decoded Prime Implicants}
\end{figure}





%%%%%%%
\section{Conclusion}
\label{sec:conclusion}

Quantification of feature thresholds can positively impact the number of valid nodes due to less empty implicants. 

%In future work, we implement the encoding for Random Forests which we described in Section~\ref{sec:approach:rf}. 
%We will study feature importance measures based on prime implicants and expect higher expressiveness of prime implicants for Random Forest classifiers. 


%%%%%%%
\bibliography{main}

\end{document}




